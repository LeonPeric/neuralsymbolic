{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe44393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import ltn\n",
    "import numpy as np\n",
    "import torch\n",
    "from dataset import DataLoader, get_mnist_dataset_for_digits_addition\n",
    "from logic import Stable_AND\n",
    "from models import LogitsToPredicate, SingleDigitClassifier\n",
    "from train import train_logic\n",
    "import seaborn as sns\n",
    "import torch.functional as F\n",
    "\n",
    "from scipy.stats import entropy\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57534526",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = get_mnist_dataset_for_digits_addition()\n",
    "train_loader = DataLoader(train_set, 32, shuffle=True)\n",
    "test_loader = DataLoader(test_set, 32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ab3c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "And = ltn.Connective(Stable_AND())\n",
    "# we use relaxed aggregators: see paper for details\n",
    "Exists = ltn.Quantifier(ltn.fuzzy_ops.AggregPMean(p=2), quantifier=\"e\")\n",
    "Forall = ltn.Quantifier(ltn.fuzzy_ops.AggregPMeanError(p=2), quantifier=\"f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d231de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dists(val_dict, color=\"C0\", xlabel=None, stat=\"frequency\", use_kde=True):\n",
    "    columns = len(val_dict)\n",
    "    fig, ax = plt.subplots(1, columns, figsize=(columns*3, 2.5))\n",
    "    fig_index = 0\n",
    "    for key in val_dict.keys():\n",
    "        key_ax = ax[fig_index%columns]\n",
    "        sns.histplot(val_dict[key], ax=key_ax, color=color, bins=50, stat=stat,\n",
    "                     kde=use_kde and ((val_dict[key].max()-val_dict[key].min())>1e-8)) # Only plot kde if there is variance\n",
    "        key_ax.set_title(f\"{key} \" + (r\"(%i $\\to$ %i)\" % (val_dict[key].shape[1], val_dict[key].shape[0]) if len(val_dict[key].shape)>1 else \"\"))\n",
    "        if xlabel is not None:\n",
    "            key_ax.set_xlabel(xlabel)\n",
    "        fig_index += 1\n",
    "    fig.subplots_adjust(wspace=0.4)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def visualize_weight_distribution(model, color=\"C0\"):\n",
    "    weights = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.endswith(\".bias\"):\n",
    "            continue\n",
    "        if \"batch_norm\" in name or \"bn\" in name:\n",
    "            continue\n",
    "        \n",
    "        key_name = f\"{' '.join(name.split('.')[2:])}\"\n",
    "        weights[key_name] = param.detach().view(-1).cpu().numpy()\n",
    "\n",
    "    ## Plotting\n",
    "    fig = plot_dists(weights, color=color, xlabel=\"Weight vals\")\n",
    "    fig.suptitle(\"Weight distribution\", fontsize=14, y=1.05)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def visualize_gradients(model, color=\"C0\", print_variance=False):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        net - Object of class BaseNetwork\n",
    "        color - Color in which we want to visualize the histogram (for easier separation of activation functions)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    small_loader = train_loader\n",
    "    operand_images, sum_label, _ = next(iter(small_loader))\n",
    "    operand_images, sum_label = operand_images.to(device), sum_label.to(device)\n",
    "    images_x = ltn.Variable(\"x\", operand_images[:, 0])\n",
    "    images_y = ltn.Variable(\"y\", operand_images[:, 1])\n",
    "    labels_z = ltn.Variable(\"z\", sum_label)\n",
    "    d_1 = ltn.Variable(\"d_1\", torch.tensor(range(10)))\n",
    "    d_2 = ltn.Variable(\"d_2\", torch.tensor(range(10)))\n",
    "\n",
    "    sat_agg = Forall(\n",
    "        ltn.diag(images_x, images_y, labels_z),\n",
    "        Exists(\n",
    "            vars=[d_1, d_2],\n",
    "            formula=And(model(images_x, d_1), model(images_y, d_2)),\n",
    "            cond_vars=[d_1, d_2, labels_z],\n",
    "            cond_fn=lambda d1, d2, z: torch.eq(d1.value + d2.value, z.value),\n",
    "        ),\n",
    "    ).value\n",
    "\n",
    "    model.zero_grad()\n",
    "\n",
    "    loss = 1.0 - sat_agg\n",
    "    loss.backward()\n",
    "    grads = {}\n",
    "    for name, params in model.named_parameters():\n",
    "        if \"weight\" in name and \"batch_norm\" not in name and \"bn\" not in name:\n",
    "            key_name = f\"{' '.join(name.split('.')[2:])}\"\n",
    "            grads[key_name] = params.grad.view(-1).cpu().clone().numpy()\n",
    "    model.zero_grad()\n",
    "\n",
    "    ## Plotting\n",
    "    fig = plot_dists(grads, color=color, xlabel=\"Grad magnitude\")\n",
    "    fig.suptitle(\"Gradient distribution\", fontsize=14, y=1.05)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    if print_variance:\n",
    "        for key in grads.keys():\n",
    "            print(f\"{key} - Variance: {np.var(grads[key])}\")\n",
    "\n",
    "def visualize_activations(model, color=\"C0\", print_variance=False):\n",
    "    model.eval()\n",
    "    small_loader = train_loader\n",
    "    operand_images, sum_label, _ = next(iter(small_loader))\n",
    "    operand_images, sum_label = operand_images.to(device), sum_label.to(device)\n",
    "\n",
    "    operand_images = operand_images.flatten(start_dim=0, end_dim=1)\n",
    "\n",
    "    # Pass one batch through the network, and calculate the gradients for the weights\n",
    "    x = operand_images\n",
    "    activations = {}\n",
    "    with torch.no_grad():\n",
    "        i = 0\n",
    "\n",
    "        for conv in model.model.logits_model.mnistconv.conv_layers:\n",
    "            x = model.model.logits_model.mnistconv.relu(conv(x))\n",
    "            activations[f\"conv_layer_{i}\"] = x.view(-1).detach().cpu().numpy()\n",
    "            x = model.model.logits_model.mnistconv.maxpool(x)\n",
    "            i += 1\n",
    "\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        for i in range(len(model.model.logits_model.mnistconv.linear_layers)):\n",
    "            x = model.model.logits_model.mnistconv.tanh(model.model.logits_model.mnistconv.batch_norm_layers[i](model.model.logits_model.mnistconv.linear_layers[i](x)))\n",
    "            activations[f\"linear_layer_{i}\"] = x.view(-1).detach().cpu().numpy()\n",
    "            i += 1\n",
    "\n",
    "        for i in range(len(model.model.logits_model.linear_layers) - 1):\n",
    "            x = model.model.logits_model.tanh(model.model.logits_model.batch_norm_layers[i](model.model.logits_model.linear_layers[i](x)))\n",
    "            activations[f\"linear_layer_{i + 1}\"] = x.view(-1).detach().cpu().numpy()\n",
    "            i += 1\n",
    "            \n",
    "        x = model.model.logits_model.linear_layers[-1](x)\n",
    "        activations[\"logits\"] = x.view(-1).detach().cpu().numpy()\n",
    "\n",
    "    \n",
    "\n",
    "    ## Plotting\n",
    "    fig = plot_dists(activations, color=color, stat=\"density\", xlabel=\"Activation vals\")\n",
    "    fig.suptitle(\"Activation distribution\", fontsize=14, y=1.05)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    if print_variance:\n",
    "        for key in activations.keys():\n",
    "            print(f\"{key} - Variance: {np.var(activations[key])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297adccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "converging_models = []\n",
    "non_converging_models = []\n",
    "\n",
    "while True:\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(i)\n",
    "        torch.cuda.manual_seed_all(i)\n",
    "\n",
    "    torch.manual_seed(i)\n",
    "    np.random.seed(i)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "    train_set, test_set = get_mnist_dataset_for_digits_addition()\n",
    "\n",
    "    # create train and test loader\n",
    "    train_loader = DataLoader(train_set, 32, shuffle=True)\n",
    "    test_loader = DataLoader(test_set, 32, shuffle=False)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    And = ltn.Connective(Stable_AND())\n",
    "    # we use relaxed aggregators: see paper for details\n",
    "    Exists = ltn.Quantifier(ltn.fuzzy_ops.AggregPMean(p=2), quantifier=\"e\")\n",
    "    Forall = ltn.Quantifier(ltn.fuzzy_ops.AggregPMeanError(p=2), quantifier=\"f\")\n",
    "\n",
    "    cnn_s_d = SingleDigitClassifier().to(device)\n",
    "    Digit_s_d = ltn.Predicate(LogitsToPredicate(cnn_s_d)).to(device)\n",
    "\n",
    "    model_copy = copy.deepcopy(Digit_s_d)\n",
    "\n",
    "    optimizer = torch.optim.Adam(Digit_s_d.parameters(), lr=0.001)\n",
    "    metrics_prl, model = train_logic(\n",
    "        Digit_s_d,\n",
    "        optimizer,\n",
    "        train_loader,\n",
    "        test_loader,\n",
    "        And,\n",
    "        Exists,\n",
    "        Forall,\n",
    "        n_epochs=1,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    if metrics_prl['test_accuracy_sum'][-1] > 0.5:\n",
    "        print(f\"Model {i} converged with test accuracy: {metrics_prl['test_accuracy_sum'][-1]}\")\n",
    "        if len(converging_models) < 10:\n",
    "            converging_models.append(model_copy)\n",
    "    else:\n",
    "        print(f\"Model {i} did not converge with test accuracy: {metrics_prl['test_accuracy_sum'][-1]}\")\n",
    "        non_converging_models.append(model_copy)\n",
    "\n",
    "    if len(converging_models) >= 10 and len(non_converging_models) >= 10:\n",
    "        break\n",
    "\n",
    "    i += 1\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af90cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiple_dists(val_dicts, color=\"C0\", xlabel=None, stat=\"probability\", use_kde=True):\n",
    "    columns = len(val_dicts[0])\n",
    "    fig, ax = plt.subplots(1, columns, figsize=(columns*3, 2.5))\n",
    "    fig_index = 0\n",
    "    for key in val_dicts[0].keys():\n",
    "        for i, val_dict in enumerate(val_dicts):\n",
    "            key_ax = ax[fig_index%columns]\n",
    "            sns.kdeplot(val_dict[key], ax=key_ax)\n",
    "        fig_index += 1\n",
    "    fig.subplots_adjust(wspace=0.4)\n",
    "    return fig\n",
    "\n",
    "def visualize_multiple_weight_distributions(models, color=\"C0\"):\n",
    "    items = []\n",
    "    for model in models:\n",
    "        weights = {}\n",
    "        for name, param in model.named_parameters():\n",
    "            if name.endswith(\".bias\"):\n",
    "                continue\n",
    "            if \"batch_norm\" in name or \"bn\" in name:\n",
    "                continue\n",
    "            \n",
    "            key_name = f\"{' '.join(name.split('.')[2:])}\"\n",
    "            weights[key_name] = param.detach().view(-1).cpu().numpy()\n",
    "        items.append(weights)\n",
    "        \n",
    "\n",
    "    ## Plotting\n",
    "    fig = plot_multiple_dists(items, color=color, xlabel=\"Weight vals\", )\n",
    "    fig.suptitle(\"Weight distribution\", fontsize=14, y=1.05)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    return items\n",
    "\n",
    "def visualize_multiple_gradients(models, color=\"C0\", print_variance=False):\n",
    "    items = []\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        small_loader = train_loader\n",
    "        operand_images, sum_label, _ = next(iter(small_loader))\n",
    "        operand_images, sum_label = operand_images.to(device), sum_label.to(device)\n",
    "        images_x = ltn.Variable(\"x\", operand_images[:, 0])\n",
    "        images_y = ltn.Variable(\"y\", operand_images[:, 1])\n",
    "        labels_z = ltn.Variable(\"z\", sum_label)\n",
    "        d_1 = ltn.Variable(\"d_1\", torch.tensor(range(10)))\n",
    "        d_2 = ltn.Variable(\"d_2\", torch.tensor(range(10)))\n",
    "\n",
    "        sat_agg = Forall(\n",
    "            ltn.diag(images_x, images_y, labels_z),\n",
    "            Exists(\n",
    "                vars=[d_1, d_2],\n",
    "                formula=And(model(images_x, d_1), model(images_y, d_2)),\n",
    "                cond_vars=[d_1, d_2, labels_z],\n",
    "                cond_fn=lambda d1, d2, z: torch.eq(d1.value + d2.value, z.value),\n",
    "            ),\n",
    "        ).value\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        loss = 1.0 - sat_agg\n",
    "        loss.backward()\n",
    "        grads = {}\n",
    "        for name, params in model.named_parameters():\n",
    "            if \"weight\" in name and \"batch_norm\" not in name and \"bn\" not in name:\n",
    "                key_name = f\"{' '.join(name.split('.')[2:])}\"\n",
    "                grads[key_name] = params.grad.view(-1).cpu().clone().numpy()\n",
    "        model.zero_grad()\n",
    "        \n",
    "        items.append(grads)\n",
    "    \n",
    "    ## Plotting\n",
    "    fig = plot_multiple_dists(items, color=color, xlabel=\"Grad magnitude\")\n",
    "    fig.suptitle(\"Gradient distribution\", fontsize=14, y=1.05)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    return items\n",
    "\n",
    "def visualize_multiple_activations(models, color=\"C0\", print_variance=False):\n",
    "    items = []\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        small_loader = train_loader\n",
    "        operand_images, sum_label, _ = next(iter(small_loader))\n",
    "        operand_images, sum_label = operand_images.to(device), sum_label.to(device)\n",
    "\n",
    "        operand_images = operand_images.flatten(start_dim=0, end_dim=1)\n",
    "\n",
    "        # Pass one batch through the network, and calculate the gradients for the weights\n",
    "        x = operand_images\n",
    "        activations = {}\n",
    "        with torch.no_grad():\n",
    "            i = 0\n",
    "\n",
    "            for conv in model.model.logits_model.mnistconv.conv_layers:\n",
    "                x = model.model.logits_model.mnistconv.relu(conv(x))\n",
    "                activations[f\"conv_layer_{i}\"] = x.view(-1).detach().cpu().numpy()\n",
    "                x = model.model.logits_model.mnistconv.maxpool(x)\n",
    "                i += 1\n",
    "\n",
    "            x = torch.flatten(x, start_dim=1)\n",
    "            for i in range(len(model.model.logits_model.mnistconv.linear_layers)):\n",
    "                x = model.model.logits_model.mnistconv.tanh(model.model.logits_model.mnistconv.batch_norm_layers[i](model.model.logits_model.mnistconv.linear_layers[i](x)))\n",
    "                activations[f\"linear_layer_{i}\"] = x.view(-1).detach().cpu().numpy()\n",
    "                i += 1\n",
    "\n",
    "            for i in range(len(model.model.logits_model.linear_layers) - 1):\n",
    "                x = model.model.logits_model.tanh(model.model.logits_model.batch_norm_layers[i](model.model.logits_model.linear_layers[i](x)))\n",
    "                activations[f\"linear_layer_{i + 1}\"] = x.view(-1).detach().cpu().numpy()\n",
    "                i += 1\n",
    "                \n",
    "            x = model.model.logits_model.linear_layers[-1](x)\n",
    "            activations[\"logits\"] = x.view(-1).detach().cpu().numpy()\n",
    "\n",
    "        items.append(activations)\n",
    "\n",
    "    ## Plotting\n",
    "    fig = plot_multiple_dists(items, color=color, stat=\"probability\", xlabel=\"Activation vals\")\n",
    "    fig.suptitle(\"Activation distribution\", fontsize=14, y=1.05)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ed94c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples_1 is a list of dictionaries, where each dictionary contains the weights of a model\n",
    "# goal is to flip it around, so that we have a dictionary of lists, where each list contains the weights of all models for a specific layer\n",
    "def flip_samples(samples):\n",
    "    flipped_samples = {}\n",
    "    for sample in samples:\n",
    "        for key in sample.keys():\n",
    "            if key not in flipped_samples:\n",
    "                flipped_samples[key] = []\n",
    "            flipped_samples[key].append(sample[key])\n",
    "    return flipped_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9c8472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the KL divergence matrix\n",
    "def compute_kl_matrix(group_A, group_B, bins='auto', epsilon=1e-10):\n",
    "    # Combine all samples to get common bin edges\n",
    "    all_samples = np.concatenate(group_A + group_B)\n",
    "    bin_edges = np.histogram_bin_edges(all_samples, bins=bins)\n",
    "    \n",
    "    nA, nB = len(group_A), len(group_B)\n",
    "    D = np.zeros((nA, nB))\n",
    "    \n",
    "    for i, a in enumerate(group_A):\n",
    "        hist_a, _ = np.histogram(a, bins=bin_edges)\n",
    "        hist_a = hist_a + epsilon\n",
    "        p = hist_a / np.sum(hist_a)\n",
    "        \n",
    "        for j, b in enumerate(group_B):\n",
    "            hist_b, _ = np.histogram(b, bins=bin_edges)\n",
    "            hist_b = hist_b + epsilon\n",
    "            q = hist_b / np.sum(hist_b)\n",
    "            \n",
    "            D[i, j] = entropy(p, q)\n",
    "    \n",
    "    return D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54043b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for kl divergence matrix where we only have one sample per group\n",
    "def compute_kl_matrix_single(group_A, group_B, bins='auto', epsilon=1e-10):\n",
    "    # Combine all samples to get common bin edges\n",
    "    all_samples = np.concatenate([group_A, group_B])\n",
    "    bin_edges = np.histogram_bin_edges(all_samples, bins=bins)\n",
    "    \n",
    "    hist_a, _ = np.histogram(group_A, bins=bin_edges)\n",
    "    hist_b, _ = np.histogram(group_B, bins=bin_edges)\n",
    "    \n",
    "    hist_a = hist_a + epsilon\n",
    "    hist_b = hist_b + epsilon\n",
    "    \n",
    "    p = hist_a / np.sum(hist_a)\n",
    "    q = hist_b / np.sum(hist_b)\n",
    "    \n",
    "    return entropy(p, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3058e2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matrix(D, title=\"KL Divergence Matrix\", xlabel=\"Group B\", ylabel=\"Group A\"):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    im = plt.imshow(D, aspect='auto', cmap='viridis', vmin=0, vmax=9.5)\n",
    "    plt.colorbar(im, label='KL Divergence')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.xticks(range(10), [f'B{j+1}' for j in range(10)])\n",
    "    plt.yticks(range(10), [f'A{i+1}' for i in range(10)])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac9c34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average of each layer\n",
    "def average_samples(samples):\n",
    "    averaged_samples = {}\n",
    "    for key in samples.keys():\n",
    "        averaged_samples[key] = np.mean(samples[key], axis=0)\n",
    "    return averaged_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6e2594",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_1 = visualize_multiple_weight_distributions(converging_models, color=\"C0\")\n",
    "samples_2 = visualize_multiple_weight_distributions(non_converging_models, color=\"C1\")\n",
    "\n",
    "# Flip the samples to have a dictionary of lists\n",
    "samples_1 = flip_samples(samples_1)\n",
    "samples_2 = flip_samples(samples_2)\n",
    "\n",
    "# average the samples\n",
    "samples_1_avg = average_samples(samples_1)\n",
    "samples_2_avg = average_samples(samples_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d844e5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_kl_divergences = []\n",
    "average_kl_divergences_inverse = []\n",
    "\n",
    "for layer in samples_1.keys():\n",
    "    average = []\n",
    "    inverse_average = []\n",
    "    # make all possible pairs of samples in the layer\n",
    "    all_pairs = [(samples_1[layer][i], samples_1[layer][j]) for i in range(len(samples_1[layer])) for j in range(i, len(samples_1[layer]))]\n",
    "    for i, j in all_pairs:\n",
    "        kl_divergence = compute_kl_matrix_single(i, j)\n",
    "        kl_divergence_inverse = compute_kl_matrix_single(j, i)\n",
    "        average.append(kl_divergence)\n",
    "        inverse_average.append(kl_divergence_inverse)\n",
    "    average_kl_divergences.append(sum(average) / (len(average)))\n",
    "    average_kl_divergences_inverse.append(sum(inverse_average) / (len(inverse_average)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d3fc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, key in enumerate(samples_1_avg.keys()):\n",
    "    group_A = samples_1_avg[key]\n",
    "    group_B = samples_2_avg[key]\n",
    "\n",
    "    kl_divergence = compute_kl_matrix_single(group_A, group_B, bins='auto', epsilon=1e-10)\n",
    "    kl_divergence_inverse = compute_kl_matrix_single(group_B, group_A, bins='auto', epsilon=1e-10)\n",
    "    print(f\"Average inner group KL divergence is: {average_kl_divergences[i]:.4f}, inverse is: {average_kl_divergences_inverse[i]:.4f}, KL divergence between converging_models and non-converging_models: {kl_divergence:.4f}, inverse is: {kl_divergence_inverse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb975e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in samples_1_avg.keys():\n",
    "    group_A = samples_1_avg[key]\n",
    "    group_B = samples_2_avg[key]\n",
    "\n",
    "    kl_divergence = compute_kl_matrix_single(group_A, group_B, bins='auto', epsilon=1e-10)\n",
    "    kl_divergence_inverse = compute_kl_matrix_single(group_B, group_A, bins='auto', epsilon=1e-10)\n",
    "    print(f\"KL Divergence for {key}: {kl_divergence:.4f}, Inverse: {kl_divergence_inverse:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eiai2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
